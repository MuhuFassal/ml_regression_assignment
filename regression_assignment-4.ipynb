{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that incorporates L1 regularization into its cost function. The cost function for Lasso is:\n",
    "\n",
    "\\[ \\text{Cost function for Lasso:} \\quad \\text{SSE} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n",
    "\n",
    "- **Lasso vs OLS**: Unlike ordinary least squares (OLS) regression, Lasso adds a penalty to the absolute values of the coefficients, which can shrink some coefficients exactly to zero.\n",
    "- **Lasso vs Ridge**: Lasso uses L1 regularization, which can lead to sparse models (i.e., models where some coefficients are zero), while Ridge uses L2 regularization, which shrinks coefficients but does not reduce any to zero.\n",
    "\n",
    "Lasso is particularly useful when feature selection is desired, as it can automatically set some coefficients to zero, effectively eliminating those features from the model.\n",
    "\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is its ability to **perform automatic feature selection** by shrinking some coefficients to exactly zero. This means that Lasso can reduce the complexity of the model by excluding irrelevant or redundant features, leading to a simpler and more interpretable model.\n",
    "\n",
    "- In datasets with many features, Lasso helps identify the most important variables by discarding the less important ones.\n",
    "\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "The interpretation of the coefficients in Lasso Regression is similar to that in OLS regression, but with additional insight:\n",
    "\n",
    "- **Zero coefficients**: A coefficient being exactly zero means the corresponding feature has been removed from the model, indicating it is not important in predicting the target variable.\n",
    "- **Non-zero coefficients**: The remaining non-zero coefficients indicate the importance of the corresponding features. However, since the coefficients are shrunk, their magnitudes may be smaller than in OLS regression, reflecting the regularization.\n",
    "\n",
    "The sign and magnitude of non-zero coefficients can still be interpreted as the expected change in the target variable for a unit change in the predictor, holding other variables constant.\n",
    "\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "The primary tuning parameter in Lasso Regression is the **regularization parameter** \\( \\lambda \\), which controls the strength of the L1 penalty applied to the model:\n",
    "\n",
    "- **\\( \\lambda = 0 \\)**: The model is equivalent to OLS, with no regularization.\n",
    "- **Small \\( \\lambda \\)**: A small value of \\( \\lambda \\) results in a model close to OLS, with minimal shrinkage.\n",
    "- **Large \\( \\lambda \\)**: A large \\( \\lambda \\) increases the penalty on the coefficients, resulting in more shrinkage and potentially driving more coefficients to zero, leading to a simpler model.\n",
    "- **Cross-validation**: The optimal \\( \\lambda \\) is typically chosen through cross-validation to balance model complexity and predictive accuracy.\n",
    "\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso regression, by itself, is a linear model. However, it can be adapted for **non-linear regression problems** by transforming the input features:\n",
    "\n",
    "1. **Polynomial Features**: You can create polynomial features (e.g., squares or higher-degree terms of the original features) and then apply Lasso to these expanded features.\n",
    "2. **Feature Engineering**: Non-linear transformations (e.g., log, exponential) of the features can be applied before using Lasso regression.\n",
    "\n",
    "These transformations allow Lasso to model non-linear relationships, even though the underlying method remains linear in terms of the coefficients.\n",
    "\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "The key differences between Ridge and Lasso Regression are:\n",
    "\n",
    "1. **Regularization Type**:\n",
    "   - **Ridge** uses L2 regularization (penalizes the sum of squared coefficients).\n",
    "   - **Lasso** uses L1 regularization (penalizes the sum of absolute coefficients).\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - **Ridge** shrinks coefficients but never sets them to exactly zero, so all features remain in the model.\n",
    "   - **Lasso** can shrink coefficients to exactly zero, which leads to automatic feature selection.\n",
    "\n",
    "3. **Multicollinearity**:\n",
    "   - Both handle multicollinearity, but **Ridge** is typically preferred for this because it tends to distribute the effect across correlated features.\n",
    "   - **Lasso** tends to select one feature from a group of highly correlated features and discard the others.\n",
    "\n",
    "4. **Model Complexity**:\n",
    "   - **Ridge** works better for models where all features contribute a little (i.e., dense solutions).\n",
    "   - **Lasso** is better for models where only a few features are important (i.e., sparse solutions).\n",
    "\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, Lasso regression can handle **multicollinearity**, but it does so differently than Ridge regression:\n",
    "\n",
    "- Lasso tends to select one feature from a group of correlated variables and assigns zero coefficients to the others, effectively removing redundant features from the model.\n",
    "- This behavior can lead to more interpretable models but may cause instability when deciding which variable to keep in highly correlated groups.\n",
    "\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "The optimal value of \\( \\lambda \\) in Lasso regression is typically selected using **cross-validation**. The process involves:\n",
    "\n",
    "1. **Grid search**: Define a range of possible \\( \\lambda \\) values.\n",
    "2. **Cross-validation**: Split the data into training and validation sets, and compute the model's performance (e.g., mean squared error) for each \\( \\lambda \\).\n",
    "3. **Optimal \\( \\lambda \\)**: Choose the \\( \\lambda \\) that minimizes the cross-validated error, balancing model complexity and prediction accuracy.\n",
    "\n",
    "This approach ensures that the chosen \\( \\lambda \\) provides the best trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
