{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Difference between Simple Linear Regression and Multiple Linear Regression\n",
    "- **Simple Linear Regression**: This model explains the relationship between two variables—one independent variable (predictor) and one dependent variable (response). The equation is:\n",
    "\n",
    "  \\[\n",
    "  Y = b_0 + b_1X\n",
    "  \\]\n",
    "\n",
    "  Example: Predicting house price based on square footage. \n",
    "  - **Y**: House price (dependent)\n",
    "  - **X**: Square footage (independent)\n",
    "\n",
    "- **Multiple Linear Regression**: In this model, the dependent variable is predicted based on two or more independent variables. The equation is:\n",
    "\n",
    "  \\[\n",
    "  Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n\n",
    "  \\]\n",
    "\n",
    "  Example: Predicting house price based on square footage, number of bedrooms, and location. \n",
    "  - **Y**: House price\n",
    "  - **X₁**: Square footage\n",
    "  - **X₂**: Number of bedrooms\n",
    "  - **X₃**: Location rating\n",
    "\n",
    "### Q2: Assumptions of Linear Regression\n",
    "1. **Linearity**: The relationship between the independent and dependent variables should be linear.\n",
    "2. **Independence**: Observations should be independent of each other.\n",
    "3. **Homoscedasticity**: Constant variance of errors (residuals).\n",
    "4. **Normality of Errors**: The residuals should follow a normal distribution.\n",
    "5. **No Multicollinearity**: Independent variables should not be highly correlated.\n",
    "\n",
    "  **How to check these assumptions**:\n",
    "  - **Linearity**: Check residual plots.\n",
    "  - **Independence**: Use Durbin-Watson test.\n",
    "  - **Homoscedasticity**: Check residual vs. fitted plot.\n",
    "  - **Normality**: Use Q-Q plot or Shapiro-Wilk test.\n",
    "  - **Multicollinearity**: Check variance inflation factor (VIF).\n",
    "\n",
    "### Q3: Interpretation of Slope and Intercept\n",
    "- **Intercept (b₀)**: The predicted value of the dependent variable when all independent variables are zero. In the house price example, if square footage = 0, the intercept would represent the baseline price of a property with no square footage.\n",
    "  \n",
    "- **Slope (b₁, b₂, etc.)**: The change in the dependent variable for a one-unit increase in the independent variable. For instance, if **b₁** = 50, it means that for each additional square foot, the price increases by $50.\n",
    "\n",
    "  **Example**: If the model is:\n",
    "\n",
    "  \\[\n",
    "  \\text{Price} = 10000 + 50 \\times \\text{Square footage}\n",
    "  \\]\n",
    "  - The intercept (10,000) suggests that a house with 0 square feet has a base price of $10,000.\n",
    "  - The slope (50) indicates that for every extra square foot, the price increases by $50.\n",
    "\n",
    "### Q4: Concept of Gradient Descent\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning. It updates the model's parameters iteratively to find the best-fit line by calculating the gradient of the cost function.\n",
    "\n",
    "In linear regression, the cost function is typically Mean Squared Error (MSE), and gradient descent minimizes this by updating the weights using the formula:\n",
    "\n",
    "\\[\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\alpha\\) is the learning rate\n",
    "- \\(J(\\theta)\\) is the cost function\n",
    "\n",
    "### Q5: Multiple Linear Regression Model\n",
    "Multiple linear regression extends the simple linear regression model to include more than one independent variable:\n",
    "\n",
    "\\[\n",
    "Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n\n",
    "\\]\n",
    "\n",
    "The primary difference is the addition of more predictor variables in multiple regression. Each predictor variable has its own slope (coefficient), and together they account for more complex relationships.\n",
    "\n",
    "### Q6: Multicollinearity in Multiple Linear Regression\n",
    "**Multicollinearity** occurs when two or more independent variables are highly correlated, leading to unstable coefficients and inflated standard errors, making it hard to determine the effect of each variable.\n",
    "\n",
    "**Detection**:\n",
    "- Variance Inflation Factor (VIF): A VIF above 5 or 10 indicates multicollinearity.\n",
    "- Correlation Matrix: Check for high correlations between independent variables.\n",
    "\n",
    "**Addressing multicollinearity**:\n",
    "- Remove or combine highly correlated variables.\n",
    "- Use regularization techniques like Ridge or Lasso regression.\n",
    "\n",
    "### Q7: Polynomial Regression Model\n",
    "Polynomial regression is an extension of linear regression that models the relationship between the dependent and independent variable as an \\(n\\)-degree polynomial:\n",
    "\n",
    "\\[\n",
    "Y = b_0 + b_1X + b_2X^2 + \\dots + b_nX^n\n",
    "\\]\n",
    "\n",
    "This allows for a curved relationship, unlike linear regression, which only models straight-line relationships.\n",
    "\n",
    "**Difference**:\n",
    "- **Linear regression** fits a straight line.\n",
    "- **Polynomial regression** fits a curved line by introducing higher-degree terms of the independent variable.\n",
    "\n",
    "### Q8: Advantages and Disadvantages of Polynomial Regression\n",
    "- **Advantages**:\n",
    "  - Can capture more complex relationships compared to linear regression.\n",
    "  - More flexibility in fitting data points.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Overfitting: Polynomial models can fit the training data too well, leading to poor generalization.\n",
    "  - Higher complexity and interpretation difficulty.\n",
    "\n",
    "**When to use polynomial regression**:\n",
    "- When the relationship between variables is nonlinear.\n",
    "- For example, in modeling growth rates that accelerate or decelerate over time, such as the spread of diseases.\n",
    "\n",
    "In contrast, **linear regression** is preferred when the relationship between variables is approximately linear and simple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
